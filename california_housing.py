# -*- coding: utf-8 -*-
"""6.5c_LAB_CALIFORNIA_HOUSING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ihfDcYlpGMd4oVzzgce4TrVWoKZpF1ob

![](https://i.imgur.com/0AUxkXt.png)

# W06_D5_LAB_CALIFORNIA_HOUSING

# California Housing Prices

> The content of this notebook is based on the book **Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd**, Chapter 2.

This dataset is based on data from 1990 California census.

## Frame the Problem

A Machine Learning pipeline for real estate investments

![](https://i.imgur.com/cgtQWuB.png)

> Pipelines
> A sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply.
>
> Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls this data and spits out its own output. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust.
>
> On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system’s performance drops.

### Select a Performance Measure

Your next step is to select a performance measure. Because this is a regression problem, the metric we choose is **Root Mean Square Error (RMSE)**

## Get the Data

You could use your web browser to download the file and run tar xzf `housing.tgz` to decompress it and extract the CSV file, but it is preferable to create a small function to do that. Having a function that downloads the data is useful in particular if the data changes regularly: you can write a small script that uses the function to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals). Automating the process of fetching the data is also useful if you need to install the dataset on multiple machines.

**Import data and libraries**
"""

#@title Import data and Libraries
# Import libaries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


import os
import tarfile
import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"
PROJECT_ROOT_DIR = "."
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images")
os.makedirs(IMAGES_PATH, exist_ok=True)

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)  
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
    
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

fetch_housing_data()
housing = load_housing_data()

#@title
# Run this to view our data
housing.head()

"""### Take a Quick Look at the Data Struture

> Run all cells below
"""

housing.info()

housing['ocean_proximity'].value_counts()

housing.describe()

"""**First Overview:**

* Each value represents  one district.
* There are ten attributes.
* `total_bedrooms` has only 20.433 nonnull values, meaning that 207 districts are missing this feature.
* All attributes are numerical, except the `ocean_proximity` field.

Next let's take a look at a histogram for each numerical attribute. You can call the `hist()` method on the whole dataset, and it will plot a histogram for each numerical attribute.
"""

housing.hist(bins=50, figsize=(20,15))
plt.show()

"""**There are a few things to notice in these histograms:**

* **Does the median income attribute look like it is expressed in USD?**

    Median income attribute is not expressed in USD. 

* **In your opinion, what is the reason for those tall bins at the end for housing median age and the median house value?** 

    Limited data in those features

* **Are the attributes having similar scales?**

  No


*Note: Many histogram are tail-heavy (skewness). This may make it harder for some ML algorithms to detect patterns. We will try transforming these attributes to have more bell-shaped distributions.*

Let's take a look at the correlation
"""

# Get the correlation matrix of dataframe
corr_matrix = housing.corr()
corr_matrix

"""### Question 1 (VALUE - 10 pts)"""

# Try to use code to find the answer don't use your eyes. 
# Hint: get the correlation column of median_house_value then sort by value

pd.DataFrame(corr_matrix['median_house_value'].sort_values(ascending=False))

"""### Create a Test Set

We can use `sklearn.train_test_split` to have a random sampling
"""

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

"""But it's not perfect. We might run the risk of introducing a significant sampling-bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population. For example, the US population is 51.3% females and 48.7% males, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population.

We know that median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset.
"""

housing["median_income"]

# Gán label classìication cho cột categorical 
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])

housing['income_cat']

housing["income_cat"].hist()

housing["income_cat"].value_counts(normalize=True)

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42, stratify=housing['income_cat'])

train_set["income_cat"].value_counts(normalize=True)

test_set["income_cat"].value_counts(normalize=True)

"""Now you should remove the income_cat attribute so the data is back to its original state:"""

train_set = train_set.drop("income_cat", axis=1)
test_set = test_set.drop("income_cat", axis=1)

"""## Discover and Visualize the Data to Gain Insights

First, make sure you have put the test set aside and you are only exploring the training set. Let's create a copy so that you can play with the training set without harming it:
"""

housing = train_set.copy()

"""### Visualize Geographical Data"""

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

housing.plot(kind='scatter', x="longitude", y="latitude")
save_fig("bad_visualization_plot")

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
save_fig("better_visualization_plot")

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
    s=housing["population"]/100, label="population", figsize=(10,7),
    c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
    sharex=False)
plt.legend()
save_fig("housing_prices_scatterplot")

# Download the California image
images_path = os.path.join(PROJECT_ROOT_DIR, "images")
os.makedirs(images_path, exist_ok=True)
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
filename = "california.png"
print("Downloading", filename)
url = DOWNLOAD_ROOT + "images/end_to_end_project/" + filename
urllib.request.urlretrieve(url, os.path.join(images_path, filename))

import matplotlib.image as mpimg
california_img=mpimg.imread(os.path.join(images_path, filename))
ax = housing.plot(kind="scatter", x="longitude", y="latitude", figsize=(10,7),
                       s=housing['population']/100, label="Population",
                       c="median_house_value", cmap=plt.get_cmap("jet"),
                       colorbar=False, alpha=0.4,
                      )
plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,
           cmap=plt.get_cmap("jet"))
plt.ylabel("Latitude", fontsize=14)
plt.xlabel("Longitude", fontsize=14)

prices = housing["median_house_value"]
tick_values = np.linspace(prices.min(), prices.max(), 11)
cbar = plt.colorbar()
cbar.ax.set_yticklabels(["$%dk"%(round(v/1000)) for v in tick_values], fontsize=14)
cbar.set_label('Median House Value', fontsize=16)

plt.legend(fontsize=16)
save_fig("california_housing_prices_plot")
plt.show()

"""Giá nhà có thể liên quan đến các đặc điểm như:
    - Mật độ dân cư
    - Vị trí gần biển

    Tuy nhiên, ở khu vực phía Bắc giá nhà lại không quá cao

### Looking for Correlations

> **WARNING:** The correlation coefficient only measures linear correlations (“if x goes up, then y generally goes up/down”). It may completely miss out on nonlinear relationships (e.g., “if x is close to 0, then y generally goes up”).
"""

corr_matrix = housing.corr()
corr_matrix['median_house_value'].sort_values(ascending=False)

"""Another way to check for correlation between attributes is to use the pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 112 = 121 plots, which would not fit on a page—so let’s just focus on a few promising attributes that seem most correlated with the median housing value"""

from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms",
              "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))

# seaborn also has a similiar function for this plot
# sns.pairplot(housing[attributes])

plt.show()

"""Median income, total rooms and age are top 3 variables in terms of correlation with our target variable. We can even look at the correlation plots"""

# The most promising attribute to predict the median house value is the median income

housing.plot(kind="scatter", x="median_income", y="median_house_value",
             alpha=0.1)

"""**This plot reveals a few things:**
* The correlation is indeed very strong.
* The price cap is clearly visible as a horizontal line at `$500,000`
* But this plot reveals other less obvious straight lines: a horizontal line around `$450,000`, another around `$350,000`

### Experimenting with Attribute Combinations

There are a few things we could try manipulating the data before training ML algorithms. Example, the number of rooms in a district is not very useful. What we really want is the number of rooms per household. Similiar with the total number of bedrooms, we probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at.
"""

housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["population_per_household"]=housing["population"]/housing["households"]

corr_matrix = housing.corr()

corr_matrix['median_house_value'].sort_values(ascending=False)

"""Not bad! The new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a **lower** bedroom/room ratio tend to be more expensive (notice the negative sign). The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.

> This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.

## Prepare the Data for ML algorithms

It’s time to prepare the data for your Machine Learning algorithms. Instead of doing this manually, you should write functions for this purpose.
"""

housing = train_set.drop("median_house_value", axis=1)
housing_labels = train_set["median_house_value"].copy()

"""### Data Cleaning

We saw earlier that the total_bedrooms attribute has some missing values, so let’s fix this. We have 3 options:

```python
housing.dropna(subset=["total_bedrooms"])    # option 1
housing.drop("total_bedrooms", axis=1)       # option 2
median = housing["total_bedrooms"].median()  # option 3
housing["total_bedrooms"].fillna(median, inplace=True)
```

Here we choose option 3, we should compute the median value on the training set and use it to fill the missing values in the training set. 

> Don’t forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.

Scikit-Learn provides a handy class to take care of missing values: `SimpleImputer`. Here is how to use it. First, you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:
"""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")

# the median can only be computed on numerical attributes
housing_num = housing.drop("ocean_proximity", axis=1)
imputer.fit(housing_num)

print(imputer.statistics_)
print(housing_num.median().values)

"""Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:"""

X = imputer.transform(housing_num)

# The result is a plain NumPy array containing the transformed features. 
# If you want to put it back into a pandas DataFrame:
pd.DataFrame(X, columns=housing_num.columns,
                          index=housing_num.index)

"""### Handling Categorical Attributes

"""

housing["ocean_proximity"].value_counts()

from sklearn.preprocessing import OrdinalEncoder

housing_cat = housing[["ocean_proximity"]]
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)

print(housing_cat[:10])
print(housing_cat_encoded[:10])
print(f"Encoder's categories {ordinal_encoder.categories_}")

from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot

"""> Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. You can use it mostly like a normal 2D array,21 but if you really want to convert it to a (dense) NumPy array, just call the toarray() method:"""

cat_encoder.categories_

# Chuyển sparse matrix thành mảng Numpy dày đặc

housing_cat_1hot.toarray()

"""### Custom Transformer

Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes.

"""

from sklearn.base import BaseEstimator, TransformerMixin

rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
        
        # concatenate newly created columns to the existing numpy array X
        return np.c_[X, rooms_per_household, population_per_household,
                      bedrooms_per_room]


attr_adder = CombinedAttributesAdder()
housing_extra_attribs = attr_adder.transform(housing.values)

housing_extra_attribs.shape

housing.shape

"""### Feature Scaling

One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales.

There are two common ways to get all attributes to have the same scale: `min-max scaling`(normalization) and `standardization`.

* Min-Max-Scaling: values are shifted so that they end up ranging from 0 to 1. Scikit-learn provides a transformer called `MinMaxScaler` for this.
* Standardization: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Standardization does not bound values to a specific range, and is much less affected by outliers. Scikit-learn provides a transformer called `StandardScaler` for this.

### Transformation Pipeline

As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for the numerical attributes:
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
                        ('imputer', SimpleImputer(strategy="median")),
                        ('attribs_adder', CombinedAttributesAdder()),
                        ('std_scaler', StandardScaler()),
                        ])

housing_num_tr = num_pipeline.fit_transform(housing_num)

"""So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column."""

from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
                                    ("num", num_pipeline, num_attribs),
                                    ("cat", OneHotEncoder(), cat_attribs),
                                  ])

housing_prepared = full_pipeline.fit_transform(housing)

"""> **Tip:** Instead of using a transformer, you can specify the string "drop" if you want the columns to be dropped, or you can specify "passthrough" if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to "passthrough") if you want these columns to be handled differently."""

housing_prepared

"""## Select and Train a Model

Now using `housing_prepared` and `housing_labels` to train model. Remember to use **Root Mean Squared Error** as your metric

### Using LinearRegression
"""

from sklearn.linear_model import LinearRegression

lin_model = LinearRegression()
lin_model.fit(housing_prepared, housing_labels)

data_prepared = full_pipeline.transform(housing)

print(f'Predict :',lin_model.predict(data_prepared))

"""Comparasion to truth values"""

print("Labels:", list(housing_labels))

data_prepared

from sklearn.metrics import mean_squared_error 

housing_preds = lin_model.predict(housing_prepared)
linear_mse = mean_squared_error(housing_labels,housing_preds)
linear_rmse = np.sqrt(linear_mse)
print( f'Root mean square error: ',linear_rmse)

from sklearn.metrics import mean_absolute_error

linear_mae = mean_absolute_error(housing_labels, housing_preds)
print( f'Mean square error: ',linear_mae)

"""### Using DecisionTreeRegressor"""

from sklearn.tree import DecisionTreeRegressor

tree_model = DecisionTreeRegressor(random_state=42)
tree_model.fit(housing_prepared, housing_labels)

tree_data_prepared = full_pipeline.transform(housing)

print(f'Predict :',tree_model.predict(tree_data_prepared))

print("Labels:", list(housing_labels))

from sklearn.metrics import mean_squared_error

tree_preds = tree_model.predict(housing_prepared)

tree_mse = mean_squared_error(tree_preds, housing_labels)

tree_rmse = np.sqrt(tree_mse)

print( f'Root Mean square error: ',tree_rmse)

"""### Better Evaluation Using Cross-Validation

One way to evaluate the Decision Tree model would be to use the train_test_split() function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. It’s a bit of work, but nothing too difficult, and it would work fairly well.

A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:

#### Question 4 (MULTICHOICE_SINGLE - 10 pts)

Tunning model
"""

# run DecisionTreeRegressor using cross_val_score
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_model, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)

tree_rmse_scores = np.sqrt(-scores)

def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

display_scores(tree_rmse_scores)

"""### Using RandomForestRegressor"""

# run RandomForestRegressor 
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)

forest_reg.fit(housing_prepared, housing_labels)

# run RandomForestRegressor using cross_val_score

housing_random_forest_predict = forest_reg.predict(housing_prepared)

scores_random_forest = mean_squared_error(housing_labels,housing_random_forest_predict)

scores_random_forest = np.sqrt(scores_random_forest)

display_scores(scores_random_forest)

# run RandomForestRegressor using cross_val_score

scores_random_forest_cross = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                             scoring='neg_mean_squared_error', cv=3)

scores_random_forest_cross = np.sqrt(-scores_random_forest_cross)

display_scores(scores_random_forest_cross)

"""Random Forests look very promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.

You should save every model you experiment with so that you can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. You can easily save Scikit-Learn models by using Python’s pickle module or by using the joblib library, which is more efficient at serializing large NumPy arrays (you can install this library using pip):
"""

import joblib

joblib.dump(forest_reg, "my_model.pkl")
# and later...
my_model_loaded = joblib.load("my_model.pkl")

"""## Fine-Tune your Model

Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.

### Grid Search

One option would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.

Instead, you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values.

> **All the things you need to do is run all the cell below in this part**
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor


param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    # try 12 (3×4) combinations of hyperparameters
  ]

forest_reg = RandomForestRegressor(random_state=42)

grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)

grid_search.fit(housing_prepared, housing_labels)

grid_search.best_params_

RandomForestRegressor(**grid_search.best_params_)

grid_search.best_estimator_

cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

pd.DataFrame(cvres)

"""### Randomized Search

The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use `RandomizedSearchCV` instead. This class can be used in much the same way as the `GridSearchCV` class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:

If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).

Simply by setting the number of iterations, you have more control over the computing budget you want to allocate to hyperparameter search.

> **All the things you need to do is run all the cell below in this part**
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {
        'n_estimators': randint(low=1, high=200),
        'max_features': randint(low=1, high=8),
        }

forest_reg = RandomForestRegressor(random_state=42)
rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,
                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
rnd_search.fit(housing_prepared, housing_labels)

for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

best_randomforest = rnd_search.best_estimator_

best_randomforest

"""### Analyze the best models and their errors

You will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions:
"""

feature_importances = grid_search.best_estimator_.feature_importances_

print(feature_importances)

"""Here is a way to extract full column names that goes along with `feature_importances`"""

extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
cat_one_hot_attribs = list(full_pipeline.named_transformers_["cat"].categories_[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
print(attributes)

"""Let’s display these importance scores next to their corresponding attribute names, and sort them in descending order in terms of the importance scores"""

sorted(zip(feature_importances, attributes), reverse=True)

# Hint: the order of weight of each feature match the order of features which you feed to the model

"""With this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others).

You should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or getting rid of uninformative ones, cleaning up outliers, etc.).

### Evaluate Your System on the Test Set

Now is the time to evaluate the final model on the test set.
"""

final_model = grid_search.best_estimator_

X_test = test_set.drop("median_house_value", axis=1)
y_test = test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)   

final_rmse

"""We can compute a 95% confidence interval for the test RMSE:"""

from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))

"""# Now, let's start with the real part of the lab today!

**Question 1**: Try adding a transformer in the preparation pipeline to select only the most important attributes.
"""

# Remember what we got feature importances already
feature_importances

# Define a function to select the indices of the top k feature importances
def indices_of_top_k(arr, k):
    return np.sort(np.argpartition(np.array(arr), -k)[-k:])

"""Let's define the number of top features we want to keep:"""

k = 8

top_k_feature_indices = indices_of_top_k(feature_importances, k)
top_k_feature_indices

"""Now let's look for the indices of the top k features:"""

np.array(attributes)[top_k_feature_indices]

"""Now let's define our TopFeatureSelector class

Note: this feature selector assumes that you have already computed the feature importances somehow (for example using a `RandomForestRegressor`). You may be tempted to compute them directly in the `TopFeatureSelector`'s `fit()` method, however this would likely slow down grid/randomized search since the feature importances would have to be computed for every hyperparameter combination (unless you implement some sort of cache).
"""

# NEW EDITED AFTER SUMMITED

from sklearn.base import BaseEstimator, TransformerMixin

def indices_of_top_k(arr, k):
    return np.sort(np.argpartition(np.array(arr), -k)[-k:])

class TopFeatureSelector(BaseEstimator, TransformerMixin):
    def __init__(self, feature_importances, k):
        self.feature_importances = feature_importances
        self.k = k
    def fit(self, X, y=None):
        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)
        return self
    def transform(self, X):
        return X[:, self.feature_indices_]

"""Looking good... Now let's create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection:"""

# NEW EDITED AFTER SUMMITED

# The pipeline should have the full pipeline and the top feature selectors you wrote above

preparation_and_feature_selection_pipeline = Pipeline([
      ('preration', full_pipeline),
      ('feature_selection', TopFeatureSelector(feature_importances, k))
])

housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)

"""Now let's look for the indices of the top k features:"""

housing_prepared_top_k_features[0:3]

"""Now let's double check that these are indeed the top k features:"""

housing_prepared[0:3, top_k_feature_indices]

"""If they are the same then it is good news!

**Question 2**: Try creating a single pipeline that does the full data preparation plus the final prediction.
"""

# NEW EDITED AFTER SUMMITED
prepare_select_and_predict_pipeline = Pipeline([
    ('preparation', full_pipeline),
    ('feature_selection', TopFeatureSelector(feature_importances, k)),
    ('tree_model', RandomForestRegressor(random_state=42))
])

prepare_select_and_predict_pipeline.fit(housing, housing_labels)

"""Let's try the full pipeline on a few instances:"""

some_data = housing.iloc[:4]
some_labels = housing_labels.iloc[:4]

print("Predictions: ", prepare_select_and_predict_pipeline.predict(some_data))
print("Labels: ", list(some_labels))

"""Well, the full pipeline seems to work fine if it can print out 5 predictions for 5 labels

**Question 3**: So far, we can do gridsearch with a ML model but can we do GridSearch for the whole pipeline, can you make GridSearchCV works with the prepare_select_and_predict_pipeline pipeline ?
"""

# To get a list of all the names for each of element in your big pipeline, use .get_params(). For example:
for i in prepare_select_and_predict_pipeline.get_params().keys():
    print(i)

# NEW EDITED AFTER SUMMITED
param_grid = [{
    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],
    'feature_selection__k': list(range(1, len(feature_importances) + 1))
}]

grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,
                                scoring='neg_mean_squared_error', verbose=2)
grid_search_prep.fit(housing, housing_labels)

# NEW EDITED AFTER SUMMITED

grid_search_prep.best_params_

# NEW EDITED AFTER SUMMITED

# get the RMSE of the 5-fold best model from this grid search
np.sqrt(-grid_search_prep.best_score_)

"""# Let's tune the model

Let's try to finetune the best possible model for this dataset. Here a few things you can do
- Try RandomizedSearchCV on different hyperparameters of RandomForestRegressor (if you only tune `max_leaf_node` from the GridSearchCV above, try tuning otherhyperparams as well. List of all the RFRegressor hyperparams can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

- Optional: Try AdaBoost or XGBoost. This is a good opportunity to get used to these models. Go back to the lecture notebook to see guides on how to finetune XGBoost.

- Report your best RMSE for this dataset!
"""